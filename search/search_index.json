{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to MkDocs For full documentation visit mkdocs.org . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Home"},{"location":"#welcome-to-mkdocs","text":"For full documentation visit mkdocs.org .","title":"Welcome to MkDocs"},{"location":"#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"PubSub_messaging_vs_Streaming/","text":"Danube Platform Danube Platform service is designed for high-performance & low-latency messaging. As for now it supports only Pub/Sub Messaging . Danube Pub/Sub messaging Purpose and Use Cases Purpose : Designed for decoupling producers and consumers, enabling asynchronous communication between different parts of a system. Use Cases : Event-driven architectures, real-time notifications, decoupled microservices, and distributed systems. Suitable for scenarios where low latency is critical and some message loss is acceptable, such as real-time monitoring, telemetry data, and ephemeral chat messages. Architecture and Design Components : Consists of Producers, Consumers (subscriptions), and the message broker. Message Flow : Producers send messages to a broker, which then distributes them to subscribers based on subscription criteria. Scaling : Scales by adding more brokers or distributing load (topics / partitions) across multiple brokers. Data Handling and Processing Models Pub/Sub messaging Producers Low Latency : Messages sent to topics are not stored on disk, which reduces the latency associated with producing messages. Order Guarantees : Provide ordering within the topic or partition. Message Delivery : There are no guarantees that messages will be delivered. If the broker crashes or if there are network issues, messages might be lost. Transient Acknowledgements : Acknowledgements to the producer are quicker since they are based on in-memory operations rather than disk writes. Publishing Without Consumers : Producers are allowed to publish messages to topics even if there are no active consumers. However, if no consumers are connected, these messages will effectively be dropped because the topics do not store messages. Pub/Sub messaging Consumers Real-time Consumption : Consumers of the topics typically process messages in real-time. If a consumer is not available, the message might be lost. No Replay : Since messages are not stored, consumers cannot replay messages. They must process them as they arrive. Reduced Overhead : The topics can handle higher throughput with lower overhead, suitable for use cases where occasional message loss is acceptable. Message Delivery : Messages are delivered to consumers only if they are currently connected and subscribed to the topic. If there are no consumers, the messages are not retained and are discarded by the broker. Order of Operations of Pub/Sub messaging Producer Publishes Message : The producer sends a message to the broker. Broker Receives Message : The broker processes the message. Consumer Availability Check : If consumers are available, the message is delivered to them in real-time. No Consumers : If no consumers are connected, the message is discarded. Danube Streaming (design considerations) Purpose and Use Cases Purpose : Designed for processing and analyzing large volumes of data in real-time as it is generated. Use Cases : Real-time analytics, data pipelines, event sourcing, continuous data processing, and stream processing applications. Ideal for use cases requiring high reliability and message durability, such as financial transactions, order processing, and logging critical events. Architecture and Design Components : Consists of producers, consumers, stream processors, and a distributed log. Data Flow : Producers write data to a distributed log, which consumers and stream processors read from in a continuous fashion. Scaling : Designed to handle high throughput and scale horizontally by partitioning data across multiple nodes in a cluster. Data Handling and Processing Models Streaming Producers Durability : Messages sent to topics are stored to persistent storage and replicated according to the topic's configuration. This ensures that messages are not lost even if brokers crash. This allows playback of streams for for historical data analysis and reprocessing. Acknowledgements : Producers receive acknowledgments once the message is safely stored and replicated. This adds a small amount of latency compared to pub/sub messaging. Order Guarantees : Provide ordering within the topic or partition. Delivery Guarantees : Producers can rely on stronger delivery guarantees (e.g., at least once or effectively once). Publishing Without Consumers : Producers can publish messages to a topic even if there are no active consumers. These messages will be stored by the broker. Processing : Supports complex processing such as windowed operations, aggregations, joins, and stateful transformations. Streaming Consumers Message Retention : Consumers can consume messages at any time as long as the retention policy allows. This is useful for replaying messages, ensuring that no messages are missed. Consumption Acknowledgements : Consumers acknowledge each message, allowing the broker to track which messages have been consumed and manage retention accordingly. Fault Tolerance : If a consumer crashes, it can resume consumption from where it left off, as the messages are stored persistently on the broker. Message Retention : Messages are stored according to the configured retention policies. This ensures that even if no consumers are currently connected, the messages will be available for consumption later. Order of Operations of Streaming Producer Publishes Message : The producer sends a message to the broker. Broker Receives and Stores Message : The broker stores the message on the persistent storage and replicates it according to the configuration. Message Acknowledgment : The broker acknowledges the producer that the message is safely stored. Consumer Availability Check : If consumers are available, the message is delivered to them. No Consumers : If no consumers are connected, the message remains stored and is available for future consumption.","title":"PubSub vs Streaming"},{"location":"PubSub_messaging_vs_Streaming/#danube-platform","text":"Danube Platform service is designed for high-performance & low-latency messaging. As for now it supports only Pub/Sub Messaging .","title":"Danube Platform"},{"location":"PubSub_messaging_vs_Streaming/#danube-pubsub-messaging","text":"","title":"Danube Pub/Sub messaging"},{"location":"PubSub_messaging_vs_Streaming/#purpose-and-use-cases","text":"Purpose : Designed for decoupling producers and consumers, enabling asynchronous communication between different parts of a system. Use Cases : Event-driven architectures, real-time notifications, decoupled microservices, and distributed systems. Suitable for scenarios where low latency is critical and some message loss is acceptable, such as real-time monitoring, telemetry data, and ephemeral chat messages.","title":"Purpose and Use Cases"},{"location":"PubSub_messaging_vs_Streaming/#architecture-and-design","text":"Components : Consists of Producers, Consumers (subscriptions), and the message broker. Message Flow : Producers send messages to a broker, which then distributes them to subscribers based on subscription criteria. Scaling : Scales by adding more brokers or distributing load (topics / partitions) across multiple brokers.","title":"Architecture and Design"},{"location":"PubSub_messaging_vs_Streaming/#data-handling-and-processing-models","text":"","title":"Data Handling and Processing Models"},{"location":"PubSub_messaging_vs_Streaming/#pubsub-messaging-producers","text":"Low Latency : Messages sent to topics are not stored on disk, which reduces the latency associated with producing messages. Order Guarantees : Provide ordering within the topic or partition. Message Delivery : There are no guarantees that messages will be delivered. If the broker crashes or if there are network issues, messages might be lost. Transient Acknowledgements : Acknowledgements to the producer are quicker since they are based on in-memory operations rather than disk writes. Publishing Without Consumers : Producers are allowed to publish messages to topics even if there are no active consumers. However, if no consumers are connected, these messages will effectively be dropped because the topics do not store messages.","title":"Pub/Sub messaging Producers"},{"location":"PubSub_messaging_vs_Streaming/#pubsub-messaging-consumers","text":"Real-time Consumption : Consumers of the topics typically process messages in real-time. If a consumer is not available, the message might be lost. No Replay : Since messages are not stored, consumers cannot replay messages. They must process them as they arrive. Reduced Overhead : The topics can handle higher throughput with lower overhead, suitable for use cases where occasional message loss is acceptable. Message Delivery : Messages are delivered to consumers only if they are currently connected and subscribed to the topic. If there are no consumers, the messages are not retained and are discarded by the broker.","title":"Pub/Sub messaging Consumers"},{"location":"PubSub_messaging_vs_Streaming/#order-of-operations-of-pubsub-messaging","text":"Producer Publishes Message : The producer sends a message to the broker. Broker Receives Message : The broker processes the message. Consumer Availability Check : If consumers are available, the message is delivered to them in real-time. No Consumers : If no consumers are connected, the message is discarded.","title":"Order of Operations of Pub/Sub messaging"},{"location":"PubSub_messaging_vs_Streaming/#danube-streaming-design-considerations","text":"","title":"Danube Streaming (design considerations)"},{"location":"PubSub_messaging_vs_Streaming/#purpose-and-use-cases_1","text":"Purpose : Designed for processing and analyzing large volumes of data in real-time as it is generated. Use Cases : Real-time analytics, data pipelines, event sourcing, continuous data processing, and stream processing applications. Ideal for use cases requiring high reliability and message durability, such as financial transactions, order processing, and logging critical events.","title":"Purpose and Use Cases"},{"location":"PubSub_messaging_vs_Streaming/#architecture-and-design_1","text":"Components : Consists of producers, consumers, stream processors, and a distributed log. Data Flow : Producers write data to a distributed log, which consumers and stream processors read from in a continuous fashion. Scaling : Designed to handle high throughput and scale horizontally by partitioning data across multiple nodes in a cluster.","title":"Architecture and Design"},{"location":"PubSub_messaging_vs_Streaming/#data-handling-and-processing-models_1","text":"","title":"Data Handling and Processing Models"},{"location":"PubSub_messaging_vs_Streaming/#streaming-producers","text":"Durability : Messages sent to topics are stored to persistent storage and replicated according to the topic's configuration. This ensures that messages are not lost even if brokers crash. This allows playback of streams for for historical data analysis and reprocessing. Acknowledgements : Producers receive acknowledgments once the message is safely stored and replicated. This adds a small amount of latency compared to pub/sub messaging. Order Guarantees : Provide ordering within the topic or partition. Delivery Guarantees : Producers can rely on stronger delivery guarantees (e.g., at least once or effectively once). Publishing Without Consumers : Producers can publish messages to a topic even if there are no active consumers. These messages will be stored by the broker. Processing : Supports complex processing such as windowed operations, aggregations, joins, and stateful transformations.","title":"Streaming Producers"},{"location":"PubSub_messaging_vs_Streaming/#streaming-consumers","text":"Message Retention : Consumers can consume messages at any time as long as the retention policy allows. This is useful for replaying messages, ensuring that no messages are missed. Consumption Acknowledgements : Consumers acknowledge each message, allowing the broker to track which messages have been consumed and manage retention accordingly. Fault Tolerance : If a consumer crashes, it can resume consumption from where it left off, as the messages are stored persistently on the broker. Message Retention : Messages are stored according to the configured retention policies. This ensures that even if no consumers are currently connected, the messages will be available for consumption later.","title":"Streaming Consumers"},{"location":"PubSub_messaging_vs_Streaming/#order-of-operations-of-streaming","text":"Producer Publishes Message : The producer sends a message to the broker. Broker Receives and Stores Message : The broker stores the message on the persistent storage and replicates it according to the configuration. Message Acknowledgment : The broker acknowledges the producer that the message is safely stored. Consumer Availability Check : If consumers are available, the message is delivered to them. No Consumers : If no consumers are connected, the message remains stored and is available for future consumption.","title":"Order of Operations of Streaming"},{"location":"Queuing_PubSub_messaging/","text":"Pub-Sub messaging Danube is built on the publish-subscribe pattern. In this pattern, producers publish messages to topics; consumers subscribe to those topics, process incoming messages, and send acknowledgments to the broker when processing is finished. Messages It is the basic unit, they are what producers publish to topics and what consumers then consume from topics. Structure: Value / data payload - The data carried by the message. The messages should contain raw bytes, and the schema, serialization / deserialization should be managed by producers / consumers. Properties - An optional key/value map of user-defined properties. Producer name - The name of the producer who produces the message. Topic name - The name of the topic that the message is published to. Sequence ID - Each message belongs to an ordered sequence on its topic.. Publish time - The timestamp of when the message is published. The default max size of a message is XX MB, that can be configured. Acknowledgment A message acknowledgment is sent by a consumer to a broker after the consumer consumes a message successfully. In the Persistent mode the consumed message will be permanently stored and deleted only after all the subscriptions have acknowledged it. Acknowledgment timeout In the Persistent mode, the acknowledgment timeout mechanism allows you to set a time range during which the client tracks the unacknowledged messages. After this acknowledgment timeout (ackTimeout) period, the client sends redeliver unacknowledged messages request to the broker, thus the broker resends the unacknowledged messages to the consumer. Topics A topic is a unit of storage that organizes messages into a stream. As in other pub-sub systems, topics are named channels for transmitting messages from producers to consumers. Topic names are URLs that have a well-defined structure: /{namespace}/{topic_name} Example: /default/markets (where default is the namespace and markets the topic) Subscriptions Pub-Sub or Queuing If you want to achieve message queuing among consumers, share the same subscription name among multiple consumers If you want to achieve traditional fan-out pub-sub messaging among consumers, specify a unique subscription name for each consumer with an exclusive subscription type. A Danube subscription is a named configuration rule that determines how messages are delivered to consumers. It is a lease on a topic established by a group of consumers: Exclusive (can be used for pub-sub) - The exclusive type is a subscription type that only allows a single consumer to attach to the subscription. If multiple consumers subscribe to a topic using the same subscription, an error occurs. Shared (for queuing) - The shared subscription type Danube allows multiple consumers to attach to the same subscription. Messages are delivered in a round-robin distribution across consumers, and any given message is delivered to only one consumer. Multi-topic subscriptions Not intended to be supported soon !.. using regex subscription. Partitioned topics Not Yet Implemented Normal topics are served only by a single broker, which limits the maximum throughput of the topic. Partitioned topic is a special type of topic handled by multiple brokers, thus allowing for higher throughput. A partitioned topic is implemented as N internal topics, where N is the number of partitions. When publishing messages to a partitioned topic, each message is routed to one of several brokers. The distribution of partitions across brokers is handled automatically. Messages for the topic are broadcast to two consumers. The routing mode determines each message should be published to which partition, while the subscription type determines which messages go to which consumers. Routing modes When publishing to partitioned topics, you must specify a routing mode. The routing mode determines each message should be published to which partition or which internal topic. RoundRobinPartition - The producer will publish messages across all partitions in round-robin fashion to achieve maximum throughput. If a key is specified on the message, the partitioned producer will hash the key and assign message to a particular partition . SinglePartition - If no key is provided, the producer will randomly pick one single partition and publish all the messages into that partition. While if a key is specified on the message, the partitioned producer will hash the key and assign message to a particular partition. ============================= Queuing vs Pub-Sub Below is some general documentation about Queuing and Pub-Sub, not related to Danube implementation, but just to ensure we are aware of the standards. Messaging queuing and pub-sub are both messaging patterns used for asynchronous communication between applications, but they differ in their approach: Queuing Messaging Model : Point-to-Point (One-to-One). A message producer sends a message to a specific queue, and only one consumer can receive and process that message. Order : Messages are typically processed in the order they are received (FIFO - First-In-First-Out). This ensures tasks are completed sequentially. Delivery : Messages are guaranteed to be delivered at least once. This reliability is crucial for critical tasks. Examples: Use cases include processing orders, sending emails, or handling failed transactions. Pub-Sub Messaging Model : Publish-Subscribe (One-to-Many). A producer publishes messages to a topic, and any interested subscribers can receive the message. Multiple subscribers can receive the same message. Order : Message order is not guaranteed. Subscribers receive messages as they are published. This is suitable for real-time updates or notifications. Delivery : Delivery is often \"fire-and-forget,\" meaning there's no guarantee a subscriber receives the message. This is acceptable for non-critical data. Examples: Use cases include broadcasting stock price updates, sending chat messages, or triggering real-time analytics. In Summary: Messaging queues are for reliable, ordered delivery to a single consumer, ideal for task processing. Pub-sub is for broadcasting messages to many interested parties, good for real-time updates.","title":"Pub-Sub messaging"},{"location":"Queuing_PubSub_messaging/#pub-sub-messaging","text":"Danube is built on the publish-subscribe pattern. In this pattern, producers publish messages to topics; consumers subscribe to those topics, process incoming messages, and send acknowledgments to the broker when processing is finished.","title":"Pub-Sub messaging"},{"location":"Queuing_PubSub_messaging/#messages","text":"It is the basic unit, they are what producers publish to topics and what consumers then consume from topics. Structure: Value / data payload - The data carried by the message. The messages should contain raw bytes, and the schema, serialization / deserialization should be managed by producers / consumers. Properties - An optional key/value map of user-defined properties. Producer name - The name of the producer who produces the message. Topic name - The name of the topic that the message is published to. Sequence ID - Each message belongs to an ordered sequence on its topic.. Publish time - The timestamp of when the message is published. The default max size of a message is XX MB, that can be configured.","title":"Messages"},{"location":"Queuing_PubSub_messaging/#acknowledgment","text":"A message acknowledgment is sent by a consumer to a broker after the consumer consumes a message successfully. In the Persistent mode the consumed message will be permanently stored and deleted only after all the subscriptions have acknowledged it.","title":"Acknowledgment"},{"location":"Queuing_PubSub_messaging/#acknowledgment-timeout","text":"In the Persistent mode, the acknowledgment timeout mechanism allows you to set a time range during which the client tracks the unacknowledged messages. After this acknowledgment timeout (ackTimeout) period, the client sends redeliver unacknowledged messages request to the broker, thus the broker resends the unacknowledged messages to the consumer.","title":"Acknowledgment timeout"},{"location":"Queuing_PubSub_messaging/#topics","text":"A topic is a unit of storage that organizes messages into a stream. As in other pub-sub systems, topics are named channels for transmitting messages from producers to consumers. Topic names are URLs that have a well-defined structure:","title":"Topics"},{"location":"Queuing_PubSub_messaging/#namespacetopic_name","text":"Example: /default/markets (where default is the namespace and markets the topic)","title":"/{namespace}/{topic_name}"},{"location":"Queuing_PubSub_messaging/#subscriptions","text":"","title":"Subscriptions"},{"location":"Queuing_PubSub_messaging/#pub-sub-or-queuing","text":"If you want to achieve message queuing among consumers, share the same subscription name among multiple consumers If you want to achieve traditional fan-out pub-sub messaging among consumers, specify a unique subscription name for each consumer with an exclusive subscription type. A Danube subscription is a named configuration rule that determines how messages are delivered to consumers. It is a lease on a topic established by a group of consumers: Exclusive (can be used for pub-sub) - The exclusive type is a subscription type that only allows a single consumer to attach to the subscription. If multiple consumers subscribe to a topic using the same subscription, an error occurs. Shared (for queuing) - The shared subscription type Danube allows multiple consumers to attach to the same subscription. Messages are delivered in a round-robin distribution across consumers, and any given message is delivered to only one consumer.","title":"Pub-Sub or Queuing"},{"location":"Queuing_PubSub_messaging/#multi-topic-subscriptions","text":"Not intended to be supported soon !.. using regex subscription.","title":"Multi-topic subscriptions"},{"location":"Queuing_PubSub_messaging/#partitioned-topics","text":"","title":"Partitioned topics"},{"location":"Queuing_PubSub_messaging/#not-yet-implemented","text":"Normal topics are served only by a single broker, which limits the maximum throughput of the topic. Partitioned topic is a special type of topic handled by multiple brokers, thus allowing for higher throughput. A partitioned topic is implemented as N internal topics, where N is the number of partitions. When publishing messages to a partitioned topic, each message is routed to one of several brokers. The distribution of partitions across brokers is handled automatically. Messages for the topic are broadcast to two consumers. The routing mode determines each message should be published to which partition, while the subscription type determines which messages go to which consumers.","title":"Not Yet Implemented"},{"location":"Queuing_PubSub_messaging/#routing-modes","text":"When publishing to partitioned topics, you must specify a routing mode. The routing mode determines each message should be published to which partition or which internal topic. RoundRobinPartition - The producer will publish messages across all partitions in round-robin fashion to achieve maximum throughput. If a key is specified on the message, the partitioned producer will hash the key and assign message to a particular partition . SinglePartition - If no key is provided, the producer will randomly pick one single partition and publish all the messages into that partition. While if a key is specified on the message, the partitioned producer will hash the key and assign message to a particular partition. =============================","title":"Routing modes"},{"location":"Queuing_PubSub_messaging/#queuing-vs-pub-sub","text":"Below is some general documentation about Queuing and Pub-Sub, not related to Danube implementation, but just to ensure we are aware of the standards. Messaging queuing and pub-sub are both messaging patterns used for asynchronous communication between applications, but they differ in their approach:","title":"Queuing vs Pub-Sub"},{"location":"Queuing_PubSub_messaging/#queuing-messaging","text":"Model : Point-to-Point (One-to-One). A message producer sends a message to a specific queue, and only one consumer can receive and process that message. Order : Messages are typically processed in the order they are received (FIFO - First-In-First-Out). This ensures tasks are completed sequentially. Delivery : Messages are guaranteed to be delivered at least once. This reliability is crucial for critical tasks. Examples: Use cases include processing orders, sending emails, or handling failed transactions.","title":"Queuing Messaging"},{"location":"Queuing_PubSub_messaging/#pub-sub-messaging_1","text":"Model : Publish-Subscribe (One-to-Many). A producer publishes messages to a topic, and any interested subscribers can receive the message. Multiple subscribers can receive the same message. Order : Message order is not guaranteed. Subscribers receive messages as they are published. This is suitable for real-time updates or notifications. Delivery : Delivery is often \"fire-and-forget,\" meaning there's no guarantee a subscriber receives the message. This is acceptable for non-critical data. Examples: Use cases include broadcasting stock price updates, sending chat messages, or triggering real-time analytics. In Summary: Messaging queues are for reliable, ordered delivery to a single consumer, ideal for task processing. Pub-sub is for broadcasting messages to many interested parties, good for real-time updates.","title":"Pub-Sub Messaging"},{"location":"architecture/","text":"Danube Architecture Danube Pub/Sub Messaging Only Danube Pub/Sub Messaging is supported for the moment Designed for decoupling producers and consumers, enabling asynchronous communication between different parts of a system. Suitable for scenarios where low latency is critical and some message loss is acceptable, such as real-time monitoring / notifications, telemetry data, event-driven architectures. The messages reside only in memory, providing low latency but not guaranteed to survive broker crashes or consumer disconnections. The producers are allowed to send messages to Topics even if there are no active consumers. If no consumers are found the messages are droped. Read Here for more detailed design considerations. Brokers A cluster consist of one or more Danube Brokers. The producers connect to the brokers to publish messages and the consumers connect to the brokers to consume the messages. Messages are dispatched immediatelly to available consumers, for increased performance. MetadataStore Used for cluster-level metadata storage, configuration and coordination. Maintain the metadata store of the Danube Cluster, such as namespace / topic metadata, broker load data and others. Danube Stream (not yet supported) Designed for processing and analyzing large volumes of data in real-time as it is generated. In Danube Stream the messages are stored durably on disk (across multiple disks for reliability). This ensures message survival even during broker restarts or consumer failures. Ideal for use cases requiring high reliability and message durability, such as financial transactions, order processing, logging critical events, etc. _Brokers A cluster consist of one or more Danube Brokers. The producers connect to the brokers to publish messages and the consumers connect to the brokers to consume the messages. Messages are typically dispatched out of a in memory cache for the sake of performance. If the backlog grows too large for the cache, the broker will start reading entries from the distributed storage. _MetadataStore Used for cluster-level metadata storage, configuration and coordination. Maintain the metadata store of the Danube Cluster, such as namespace / topic metadata, broker load data and others. Message Store It provides message delivery guarantee for applications. If a message successfully reaches the Danube broker, it will be delivered to its intended target. This guarantee requires that non-acknowledged messages are stored durably until they can be delivered to and acknowledged by consumers. This mode of messaging is commonly called persistent messaging.","title":"Danube architecture"},{"location":"architecture/#danube-architecture","text":"","title":"Danube Architecture"},{"location":"architecture/#danube-pubsub-messaging","text":"","title":"Danube Pub/Sub Messaging"},{"location":"architecture/#only-danube-pubsub-messaging-is-supported-for-the-moment","text":"Designed for decoupling producers and consumers, enabling asynchronous communication between different parts of a system. Suitable for scenarios where low latency is critical and some message loss is acceptable, such as real-time monitoring / notifications, telemetry data, event-driven architectures. The messages reside only in memory, providing low latency but not guaranteed to survive broker crashes or consumer disconnections. The producers are allowed to send messages to Topics even if there are no active consumers. If no consumers are found the messages are droped. Read Here for more detailed design considerations.","title":"Only Danube Pub/Sub Messaging is supported for the moment"},{"location":"architecture/#brokers","text":"A cluster consist of one or more Danube Brokers. The producers connect to the brokers to publish messages and the consumers connect to the brokers to consume the messages. Messages are dispatched immediatelly to available consumers, for increased performance.","title":"Brokers"},{"location":"architecture/#metadatastore","text":"Used for cluster-level metadata storage, configuration and coordination. Maintain the metadata store of the Danube Cluster, such as namespace / topic metadata, broker load data and others.","title":"MetadataStore"},{"location":"architecture/#danube-stream-not-yet-supported","text":"Designed for processing and analyzing large volumes of data in real-time as it is generated. In Danube Stream the messages are stored durably on disk (across multiple disks for reliability). This ensures message survival even during broker restarts or consumer failures. Ideal for use cases requiring high reliability and message durability, such as financial transactions, order processing, logging critical events, etc.","title":"Danube Stream (not yet supported)"},{"location":"architecture/#_brokers","text":"A cluster consist of one or more Danube Brokers. The producers connect to the brokers to publish messages and the consumers connect to the brokers to consume the messages. Messages are typically dispatched out of a in memory cache for the sake of performance. If the backlog grows too large for the cache, the broker will start reading entries from the distributed storage.","title":"_Brokers"},{"location":"architecture/#_metadatastore","text":"Used for cluster-level metadata storage, configuration and coordination. Maintain the metadata store of the Danube Cluster, such as namespace / topic metadata, broker load data and others.","title":"_MetadataStore"},{"location":"architecture/#message-store","text":"It provides message delivery guarantee for applications. If a message successfully reaches the Danube broker, it will be delivered to its intended target. This guarantee requires that non-acknowledged messages are stored durably until they can be delivered to and acknowledged by consumers. This mode of messaging is commonly called persistent messaging.","title":"Message Store"},{"location":"internal_danube_services/","text":"Danube Cluster Services Role This document enumerates the principal components of the Danube Cluster and their responsibilities. Danube Service Components Leader Election Service The Leader Election Service selects one broker from the cluster to act as the Leader. The Broker Leader is responsible for making decisions. This service is used by the Load Manager, ensuring only one broker in the cluster posts the cluster aggregated Load Report. Leader Election Flow: The first broker registered in the cluster becomes the Leader by registering itself on \"/cluster/leader\". The field is registered with a lease, so the leader broker must periodically renew its lease to maintain leadership. Subsequent brokers attempt to become leaders but become Followers if the path is already in use. All brokers periodically check the leader path. If there is no change, the state is maintained; otherwise, brokers attempt to become the leader. Load Manager Service The Load Manager monitors and distributes load across brokers by managing topic and partition assignments. It implements rebalancing logic to redistribute topics/partitions when brokers join or leave the cluster and is responsible for failover mechanisms to handle broker failures. Load Manager Flow: All brokers periodically post their Load Reports on the path \"/cluster/brokers/load/{broker-id}\". The leader broker watches for load reports from all brokers in the cluster. It calculates rankings using the selected Load Balance algorithm. It posts its calculations for the cluster on the \"/cluster/load_balance\" path. Creation of a New Topic: A broker registers the Topic on the \"/cluster/unassigned\" path. The Load Manager of the leader Broker watches this path and assigns the broker with the least load to host the new topic by posting the topic to the \"/cluster/brokers/{broker-id}/{topic_name}\" path. Each broker watches its own path: \"/cluster/brokers/{broker-id}\". For any event on that path, such as the addition or deletion of topics, it acts accordingly by creating a new topic locally or deleting the topic it owned and all related resources. On topic creation, the broker checks if the topic already exists locally. If not, it retrieves all data about the topic, including subscriptions and producers, from the Local Metadata Cache. On topic removal, the broker handles the disconnections of producers and consumers and removes the locally allocated resources. For further consideration: We may want the broker to ask the Load Manager to get the next broker and initiate topic creation. Either it just posts the topic on the \"/cluster/unassigned\" path, or if it is the selected broker, it also creates the topic locally. Local Metadata Cache This cache stores various types of metadata required by Danube brokers, such as topic and namespace data, which are frequently accessed during message production and consumption. This reduces the need for frequent queries to the central metadata store, ETCD. The docs/internal_resources.md document describes how the resources are organized in the Metadata Store. Updates/events are received via ETCD Watch events and/or the metadata event synchronizer. Syncronizer The synchronizer ensures that metadata and configuration settings across different brokers remain consistent. It propagates changes to metadata and configuration settings using client Producers and Consumers. This is in addition to Metadata Storage watch events, allowing brokers to process metadata updates even if there was a communication glitch or the broker was unavailable for a short period, potentially missing the Store Watch events. The synchronizer allows for dynamic updates to configuration settings without requiring a broker service restart. Danube Broker The Broker owns the topics and manages their lifecycle. It also facilitates the creation of producers, subscriptions, and consumers, ensuring that producers can publish messages to topics and consumers can consume messages from topics. External Metadata Storage (ETCD) This is the Metadata Storage responsible for the persistent storage of metadata and cluster synchronization.","title":"Danube service components"},{"location":"internal_danube_services/#danube-cluster-services-role","text":"This document enumerates the principal components of the Danube Cluster and their responsibilities.","title":"Danube Cluster Services Role"},{"location":"internal_danube_services/#danube-service-components","text":"","title":"Danube Service Components"},{"location":"internal_danube_services/#leader-election-service","text":"The Leader Election Service selects one broker from the cluster to act as the Leader. The Broker Leader is responsible for making decisions. This service is used by the Load Manager, ensuring only one broker in the cluster posts the cluster aggregated Load Report. Leader Election Flow: The first broker registered in the cluster becomes the Leader by registering itself on \"/cluster/leader\". The field is registered with a lease, so the leader broker must periodically renew its lease to maintain leadership. Subsequent brokers attempt to become leaders but become Followers if the path is already in use. All brokers periodically check the leader path. If there is no change, the state is maintained; otherwise, brokers attempt to become the leader.","title":"Leader Election Service"},{"location":"internal_danube_services/#load-manager-service","text":"The Load Manager monitors and distributes load across brokers by managing topic and partition assignments. It implements rebalancing logic to redistribute topics/partitions when brokers join or leave the cluster and is responsible for failover mechanisms to handle broker failures. Load Manager Flow: All brokers periodically post their Load Reports on the path \"/cluster/brokers/load/{broker-id}\". The leader broker watches for load reports from all brokers in the cluster. It calculates rankings using the selected Load Balance algorithm. It posts its calculations for the cluster on the \"/cluster/load_balance\" path. Creation of a New Topic: A broker registers the Topic on the \"/cluster/unassigned\" path. The Load Manager of the leader Broker watches this path and assigns the broker with the least load to host the new topic by posting the topic to the \"/cluster/brokers/{broker-id}/{topic_name}\" path. Each broker watches its own path: \"/cluster/brokers/{broker-id}\". For any event on that path, such as the addition or deletion of topics, it acts accordingly by creating a new topic locally or deleting the topic it owned and all related resources. On topic creation, the broker checks if the topic already exists locally. If not, it retrieves all data about the topic, including subscriptions and producers, from the Local Metadata Cache. On topic removal, the broker handles the disconnections of producers and consumers and removes the locally allocated resources. For further consideration: We may want the broker to ask the Load Manager to get the next broker and initiate topic creation. Either it just posts the topic on the \"/cluster/unassigned\" path, or if it is the selected broker, it also creates the topic locally.","title":"Load Manager Service"},{"location":"internal_danube_services/#local-metadata-cache","text":"This cache stores various types of metadata required by Danube brokers, such as topic and namespace data, which are frequently accessed during message production and consumption. This reduces the need for frequent queries to the central metadata store, ETCD. The docs/internal_resources.md document describes how the resources are organized in the Metadata Store. Updates/events are received via ETCD Watch events and/or the metadata event synchronizer.","title":"Local Metadata Cache"},{"location":"internal_danube_services/#syncronizer","text":"The synchronizer ensures that metadata and configuration settings across different brokers remain consistent. It propagates changes to metadata and configuration settings using client Producers and Consumers. This is in addition to Metadata Storage watch events, allowing brokers to process metadata updates even if there was a communication glitch or the broker was unavailable for a short period, potentially missing the Store Watch events. The synchronizer allows for dynamic updates to configuration settings without requiring a broker service restart.","title":"Syncronizer"},{"location":"internal_danube_services/#danube-broker","text":"The Broker owns the topics and manages their lifecycle. It also facilitates the creation of producers, subscriptions, and consumers, ensuring that producers can publish messages to topics and consumers can consume messages from topics.","title":"Danube Broker"},{"location":"internal_danube_services/#external-metadata-storage-etcd","text":"This is the Metadata Storage responsible for the persistent storage of metadata and cluster synchronization.","title":"External Metadata Storage (ETCD)"},{"location":"internal_resources/","text":"Resources mapping This document describes how the resources are organized in the Metadata store MetadataStore and LocalCache Basically the entire configuration and the metadata for all the cluster's objects (topics, namespaces, etc) are stored in MetadataStorage (ETCD) and in the LocalCache to ensure fast retrieval for the local broker and to reduce the number of request to the metadata database. The pattern: Put / Delete requests should use MetadataStore (ETCD), to ensure consistency across cluster Get requests should be served from the Local Cache The LocalCache continuously update from 2 sources for increase consistency: the Watch operation on ETCD the Syncronizer topic, where all Put / Delete requests are published and read by the brokers. Resources Types Cluster Resources Holds information about the cluster and the cluster's brokers. Mainly read and write by Danube Service. /cluster/cluster-name holds a String with the name of the cluster /cluster/register/{broker-id} the broker register once it join the cluster, contain the broker metadata (broker id & socket addr) /cluster/brokers/{broker-id}/{namespace}/{topic} topics served by the broker, with value () Load Manager updates the path, with topic assignments to brokers Brokers should watch it's own path like (/cluster/brokers/1122334455) - and perform the neccesary actions on adding or removing a topic /cluster/unassigned/{namespace}/{topic} New unassigned topics created by Broker Load Manager should watch this path, add assign the topic to a broker /cluster/load/{broker-id} broker periodically reports its load metrics on this path Load Manager watch this path to calculate broker load rankings for the cluster /cluster/load_balance the load_balance updated decision, posted by the Load Manager, contain a HashMap with keys the broker_id and value the list of topic_name /cluster/leader the value posted by Leader Election service, it holds broker_id of the current Leader of the CLuster Example: /cluster/brokers/1122334455/markets/trade-events - value is () /cluster/brokers/1122334455/markets/trade-events - value is () Namespace Resources Holds information about the namespace policy and the namespace's topics /namespaces/{namespace}/policy /namespaces/{namespace}/topics/{namespace}/{topic} Example: /namespaces/markets/policy - the value stores a Json like { \"retentionTimeInMinutes\": 1440 } /namespaces/markets/topics/markets/trade-events - topics part of the namespace, value is () Topic Resources Holds information about the topic policy and the associated producers / subscriptions, including partitioned topic. /topics/{namespace}/{topic}/policy holds the topic policy, the value stores a Json /topics/{namespace}/{topic}/schema holds the topic schema, the value stores the schema /topics/{namespace}/{topic}/producers/{producer_id} holds the producer config /topics/{namespace}/{topic}/subscriptions/{subscription_name} holds the subscription config Example: /topics/markets/trade-events/producers/1122334455 - with value Producer Metadata /topics/markets/trade-events/subscriptions/my_subscription - with value Subscription Metadata /topics/markets/trade-events-part-1/policy - where /markets/trade-events-part-1 is the partitioned topic that stores partition policy Subscriptions Resources Holds information about the topic subscriptions, including associated consumers /subscriptions/{subscription_name}/{consumer_id} holds the consumer metadata Example: /subscriptions/my_subscription/23232323","title":"Resources mapping"},{"location":"internal_resources/#resources-mapping","text":"This document describes how the resources are organized in the Metadata store","title":"Resources mapping"},{"location":"internal_resources/#metadatastore-and-localcache","text":"Basically the entire configuration and the metadata for all the cluster's objects (topics, namespaces, etc) are stored in MetadataStorage (ETCD) and in the LocalCache to ensure fast retrieval for the local broker and to reduce the number of request to the metadata database. The pattern: Put / Delete requests should use MetadataStore (ETCD), to ensure consistency across cluster Get requests should be served from the Local Cache The LocalCache continuously update from 2 sources for increase consistency: the Watch operation on ETCD the Syncronizer topic, where all Put / Delete requests are published and read by the brokers.","title":"MetadataStore and LocalCache"},{"location":"internal_resources/#resources-types","text":"","title":"Resources Types"},{"location":"internal_resources/#cluster-resources","text":"Holds information about the cluster and the cluster's brokers. Mainly read and write by Danube Service. /cluster/cluster-name holds a String with the name of the cluster /cluster/register/{broker-id} the broker register once it join the cluster, contain the broker metadata (broker id & socket addr) /cluster/brokers/{broker-id}/{namespace}/{topic} topics served by the broker, with value () Load Manager updates the path, with topic assignments to brokers Brokers should watch it's own path like (/cluster/brokers/1122334455) - and perform the neccesary actions on adding or removing a topic /cluster/unassigned/{namespace}/{topic} New unassigned topics created by Broker Load Manager should watch this path, add assign the topic to a broker /cluster/load/{broker-id} broker periodically reports its load metrics on this path Load Manager watch this path to calculate broker load rankings for the cluster /cluster/load_balance the load_balance updated decision, posted by the Load Manager, contain a HashMap with keys the broker_id and value the list of topic_name /cluster/leader the value posted by Leader Election service, it holds broker_id of the current Leader of the CLuster Example: /cluster/brokers/1122334455/markets/trade-events - value is () /cluster/brokers/1122334455/markets/trade-events - value is ()","title":"Cluster Resources"},{"location":"internal_resources/#namespace-resources","text":"Holds information about the namespace policy and the namespace's topics /namespaces/{namespace}/policy /namespaces/{namespace}/topics/{namespace}/{topic} Example: /namespaces/markets/policy - the value stores a Json like { \"retentionTimeInMinutes\": 1440 } /namespaces/markets/topics/markets/trade-events - topics part of the namespace, value is ()","title":"Namespace Resources"},{"location":"internal_resources/#topic-resources","text":"Holds information about the topic policy and the associated producers / subscriptions, including partitioned topic. /topics/{namespace}/{topic}/policy holds the topic policy, the value stores a Json /topics/{namespace}/{topic}/schema holds the topic schema, the value stores the schema /topics/{namespace}/{topic}/producers/{producer_id} holds the producer config /topics/{namespace}/{topic}/subscriptions/{subscription_name} holds the subscription config Example: /topics/markets/trade-events/producers/1122334455 - with value Producer Metadata /topics/markets/trade-events/subscriptions/my_subscription - with value Subscription Metadata /topics/markets/trade-events-part-1/policy - where /markets/trade-events-part-1 is the partitioned topic that stores partition policy","title":"Topic Resources"},{"location":"internal_resources/#subscriptions-resources","text":"Holds information about the topic subscriptions, including associated consumers /subscriptions/{subscription_name}/{consumer_id} holds the consumer metadata Example: /subscriptions/my_subscription/23232323","title":"Subscriptions Resources"},{"location":"producers_consumers/","text":"Danube Clients Client Setup Before an application creates a producer/consumer, the client library needs to initiate a setup phase including two steps: The client attempts to determine the owner of the topic by sending a Lookup request to Broker. Once the client library has the broker address, it creates a RPC connection (or reuses an existing connection from the pool) and (in later stage authenticates it ). Within this connection, the clients (producer, consumer) and brokers exchange RPC commands. At this point, the client sends a command to create producer/consumer to the broker, which will comply after doing some validation checks. TODO: Reconnection with exponential backoff .Whether the TCP connection breaks, the client immediately re-initiates this setup phase and keeps trying to re-establish the producer or consumer until the operation succeeds. Producer A producer is a process that attaches to a topic and publishes messages to a Danube broker. The Danube broker processes the messages. Access Mode is a mechanism to determin the permissions of producers on topics. Shared - Multiple producers can publish on a topic. Exclusive - If there is already a producer connected, other producers trying to publish on this topic get errors immediately. Example let client = DanubeClient::builder().service_url(\"http://[::1]:6650\").build().unwrap(); let topic = \"/markets/trade-events\".to_string(); // Topics are defined as /{namespace}/{tiopic_name} let json_schema = r#\"{\"type\": \"object\", \"properties\": {\"field1\": {\"type\": \"string\"}, \"field2\": {\"type\": \"integer\"}}}\"#.to_string(); let mut producer = client.new_producer().with_topic(topic).with_name(\"test_producer1\") .with_schema(\"my_app\".into(), SchemaType::Json(json_schema)).build(); producer.create().await?; let data = json!({ \"field1\": format!{\"value{}\", i}, \"field2\": 2020+i, }); // Convert to string and encode to bytes let json_string = serde_json::to_string(&data).unwrap(); let encoded_data = json_string.as_bytes().to_vec(); producer.send(encoded_data).await?; Consumer A consumer is a process that attaches to a topic via a subscription and then receives messages. Subscription Types - describe the way the consumers receive the messages from topics Exclusive - Only one consumer can subscribe, guaranteeing message order. Shared - Multiple consumers can subscribe, messages are delivered round-robin, offering good scalability but no order guarantee. Failover - Similar to shared subscriptions, but multiple consumers can subscribe, and one actively receives messages. let client = DanubeClient::builder().service_url(\"http://[::1]:6650\").build().unwrap(); let topic = \"/markets/trade-events\".to_string(); // Topics are defined as /{namespace}/{tiopic_name} let mut consumer = client.new_consumer().with_topic(topic).with_consumer_name(\"test_consumer\") .with_subscription(\"test_subscription\").with_subscription_type(SubType::Exclusive).build(); // Subscribe to the topic consumer.subscribe().await?; // Start receiving messages let mut message_stream = consumer.receive().await?; while let Some(message) = message_stream.next().await { //process the message and ack for receive }","title":"Producers Consumers"},{"location":"producers_consumers/#danube-clients","text":"","title":"Danube Clients"},{"location":"producers_consumers/#client-setup","text":"Before an application creates a producer/consumer, the client library needs to initiate a setup phase including two steps: The client attempts to determine the owner of the topic by sending a Lookup request to Broker. Once the client library has the broker address, it creates a RPC connection (or reuses an existing connection from the pool) and (in later stage authenticates it ). Within this connection, the clients (producer, consumer) and brokers exchange RPC commands. At this point, the client sends a command to create producer/consumer to the broker, which will comply after doing some validation checks. TODO: Reconnection with exponential backoff .Whether the TCP connection breaks, the client immediately re-initiates this setup phase and keeps trying to re-establish the producer or consumer until the operation succeeds.","title":"Client Setup"},{"location":"producers_consumers/#producer","text":"A producer is a process that attaches to a topic and publishes messages to a Danube broker. The Danube broker processes the messages. Access Mode is a mechanism to determin the permissions of producers on topics. Shared - Multiple producers can publish on a topic. Exclusive - If there is already a producer connected, other producers trying to publish on this topic get errors immediately.","title":"Producer"},{"location":"producers_consumers/#example","text":"let client = DanubeClient::builder().service_url(\"http://[::1]:6650\").build().unwrap(); let topic = \"/markets/trade-events\".to_string(); // Topics are defined as /{namespace}/{tiopic_name} let json_schema = r#\"{\"type\": \"object\", \"properties\": {\"field1\": {\"type\": \"string\"}, \"field2\": {\"type\": \"integer\"}}}\"#.to_string(); let mut producer = client.new_producer().with_topic(topic).with_name(\"test_producer1\") .with_schema(\"my_app\".into(), SchemaType::Json(json_schema)).build(); producer.create().await?; let data = json!({ \"field1\": format!{\"value{}\", i}, \"field2\": 2020+i, }); // Convert to string and encode to bytes let json_string = serde_json::to_string(&data).unwrap(); let encoded_data = json_string.as_bytes().to_vec(); producer.send(encoded_data).await?;","title":"Example"},{"location":"producers_consumers/#consumer","text":"A consumer is a process that attaches to a topic via a subscription and then receives messages. Subscription Types - describe the way the consumers receive the messages from topics Exclusive - Only one consumer can subscribe, guaranteeing message order. Shared - Multiple consumers can subscribe, messages are delivered round-robin, offering good scalability but no order guarantee. Failover - Similar to shared subscriptions, but multiple consumers can subscribe, and one actively receives messages. let client = DanubeClient::builder().service_url(\"http://[::1]:6650\").build().unwrap(); let topic = \"/markets/trade-events\".to_string(); // Topics are defined as /{namespace}/{tiopic_name} let mut consumer = client.new_consumer().with_topic(topic).with_consumer_name(\"test_consumer\") .with_subscription(\"test_subscription\").with_subscription_type(SubType::Exclusive).build(); // Subscribe to the topic consumer.subscribe().await?; // Start receiving messages let mut message_stream = consumer.receive().await?; while let Some(message) = message_stream.next().await { //process the message and ack for receive }","title":"Consumer"}]}