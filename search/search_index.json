{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Danube Pub/Sub messaging docs","text":"<p>Danube is an open-source, distributed publish-subscribe (Pub/Sub) message broker system developed in Rust. Inspired by the Apache Pulsar messaging and streaming platform, Danube incorporates some similar concepts but is designed to carve its own path within the distributed messaging ecosystem.</p> <p>Currently, the Danube platform exclusively supports Non-persistent messages. Meaning that  the messages reside solely in memory and are promptly distributed to consumers if they are available, utilizing a dispatch mechanism based on subscription types.</p> <p>\u26a0\ufe0f The messsaging platform is currently under active development and may have missing or incomplete functionalities. Use with caution.</p> <p>I'm continuously working on enhancing and adding new features. Contributions are welcome, and you can also report any issues you encounter. The client library is currently written in Rust, with a Go client potentially coming soon. Contributions in other languages, such as Python, Java, etc., are also greatly appreciated.</p>"},{"location":"admin_API/danube_cli/brokers/","title":"danube-cli: Brokers Commands","text":"<p>The <code>danube-cli</code> tool provides commands to manage and view information about brokers in your Danube cluster. Below is the documentation for the commands related to brokers.</p>"},{"location":"admin_API/danube_cli/brokers/#commands","title":"Commands","text":""},{"location":"admin_API/danube_cli/brokers/#danube-cli-brokers-list","title":"<code>danube-cli brokers list</code>","text":"<p>List all active brokers in the cluster.</p> <p>Usage:</p> <pre><code>danube-cli brokers list\n</code></pre> <p>Description:</p> <p>This command retrieves and displays a list of all active brokers in the cluster. The output is formatted into a table with the following columns:</p> <ul> <li>BROKER ID: The unique identifier for the broker.</li> <li>BROKER ADDRESS: The network address of the broker.</li> <li>BROKER ROLE: The role assigned to the broker (e.g., \"leader\", \"follower\").</li> </ul> <p>Example Output:</p> <pre><code>+------------+---------------------+-------------+\n| BROKER ID  | BROKER ADDRESS      | BROKER ROLE |\n+------------+---------------------+-------------+\n| 1          | 192.168.1.1:6650    | leader      |\n| 2          | 192.168.1.2:6650    | follower    |\n+------------+---------------------+-------------+\n</code></pre>"},{"location":"admin_API/danube_cli/brokers/#danube-cli-brokers-leader-broker","title":"<code>danube-cli brokers leader-broker</code>","text":"<p>Get information about the leader broker in the cluster.</p> <p>Usage:</p> <pre><code>danube-cli brokers leader-broker\n</code></pre> <p>Description:</p> <p>This command fetches and displays the details of the current leader broker in the cluster. The information includes the broker ID, address, and role of the leader.</p> <p>Example Output:</p> <pre><code>Leader Broker: BrokerId: 1, Address: 192.168.1.1:6650, Role: leader\n</code></pre>"},{"location":"admin_API/danube_cli/brokers/#danube-cli-brokers-namespaces","title":"<code>danube-cli brokers namespaces</code>","text":"<p>List all namespaces in the cluster.</p> <p>Usage:</p> <pre><code>danube-cli brokers namespaces\n</code></pre> <p>Description:</p> <p>This command retrieves and lists all namespaces associated with the cluster. Each namespace is printed on a new line.</p> <p>Example Output:</p> <pre><code>Namespace: default\nNamespace: public\nNamespace: my-namespace\n</code></pre>"},{"location":"admin_API/danube_cli/brokers/#error-handling","title":"Error Handling","text":"<p>If there is an issue with connecting to the cluster or processing the request, the CLI will output an error message. Make sure your Danube cluster is running and accessible, and check your network connectivity.</p>"},{"location":"admin_API/danube_cli/brokers/#examples","title":"Examples","text":"<p>Here are a few example commands for quick reference:</p> <ul> <li>List all brokers:</li> </ul> <pre><code>danube-cli brokers list\n</code></pre> <ul> <li>Get the leader broker:</li> </ul> <pre><code>danube-cli brokers leader-broker\n</code></pre> <ul> <li>List all namespaces:</li> </ul> <pre><code>danube-cli brokers namespaces\n</code></pre> <p>For more detailed information or help with the <code>danube-cli</code>, you can use the <code>--help</code> flag with any command.</p> <p>Example:</p> <pre><code>danube-cli brokers --help\n</code></pre>"},{"location":"admin_API/danube_cli/namespaces/","title":"danube-cli: Namespaces Commands","text":"<p>The <code>danube-cli</code> tool provides commands to manage and view information about namespaces in your Danube cluster. Below is the documentation for the commands related to namespaces.</p>"},{"location":"admin_API/danube_cli/namespaces/#commands","title":"Commands","text":""},{"location":"admin_API/danube_cli/namespaces/#danube-cli-namespaces-topics-namespace","title":"<code>danube-cli namespaces topics NAMESPACE</code>","text":"<p>Get the list of topics for a specified namespace.</p> <p>Usage:</p> <pre><code>danube-cli namespaces topics NAMESPACE\n</code></pre> <p>Description:</p> <p>This command retrieves and displays all topics associated with a specific namespace. Replace <code>NAMESPACE</code> with the name of the namespace you want to query.</p> <p>Example Output:</p> <pre><code>Topic: topic1\nTopic: topic2\nTopic: topic3\n</code></pre>"},{"location":"admin_API/danube_cli/namespaces/#danube-cli-namespaces-policies-namespace","title":"<code>danube-cli namespaces policies NAMESPACE</code>","text":"<p>Get the configuration policies for a specified namespace.</p> <p>Usage:</p> <pre><code>danube-cli namespaces policies NAMESPACE\n</code></pre> <p>Description:</p> <p>This command fetches and displays the configuration policies for a specific namespace. Replace <code>NAMESPACE</code> with the name of the namespace you want to query.</p> <p>Example Output:</p> <pre><code>Policy Name: policy1\nPolicy Description: Description of policy1\nPolicy Name: policy2\nPolicy Description: Description of policy2\n</code></pre>"},{"location":"admin_API/danube_cli/namespaces/#danube-cli-namespaces-create-namespace","title":"<code>danube-cli namespaces create NAMESPACE</code>","text":"<p>Create a new namespace.</p> <p>Usage:</p> <pre><code>danube-cli namespaces create NAMESPACE\n</code></pre> <p>Description:</p> <p>This command creates a new namespace with the specified name. Replace <code>NAMESPACE</code> with the desired name for the new namespace.</p> <p>Example Output:</p> <pre><code>Namespace Created: true\n</code></pre>"},{"location":"admin_API/danube_cli/namespaces/#danube-cli-namespaces-delete-namespace","title":"<code>danube-cli namespaces delete NAMESPACE</code>","text":"<p>Delete a specified namespace. The namespace must be empty.</p> <p>Usage:</p> <pre><code>danube-cli namespaces delete NAMESPACE\n</code></pre> <p>Description:</p> <p>This command deletes a namespace. The specified namespace must be empty before it can be deleted. Replace <code>NAMESPACE</code> with the name of the namespace you wish to delete.</p> <p>Example Output:</p> <pre><code>Namespace Deleted: true\n</code></pre>"},{"location":"admin_API/danube_cli/namespaces/#error-handling","title":"Error Handling","text":"<p>If there is an issue with connecting to the cluster or processing the request, the CLI will output an error message. Make sure your Danube cluster is running and accessible, and check your network connectivity.</p>"},{"location":"admin_API/danube_cli/namespaces/#examples","title":"Examples","text":"<p>Here are a few example commands for quick reference:</p> <ul> <li>List all topics in a namespace:</li> </ul> <pre><code>danube-cli namespaces topics my-namespace\n</code></pre> <ul> <li>Get the policies for a namespace:</li> </ul> <pre><code>danube-cli namespaces policies my-namespace\n</code></pre> <ul> <li>Create a new namespace:</li> </ul> <pre><code>danube-cli namespaces create my-new-namespace\n</code></pre> <ul> <li>Delete a namespace:</li> </ul> <pre><code>danube-cli namespaces delete my-old-namespace\n</code></pre> <p>For more detailed information or help with the <code>danube-cli</code>, you can use the <code>--help</code> flag with any command.</p> <p>Example:</p> <pre><code>danube-cli namespaces --help\n</code></pre>"},{"location":"admin_API/danube_cli/topics/","title":"danube-cli: Topics Commands","text":"<p>The <code>danube-cli</code> tool provides commands to manage and view information about topics in your Danube cluster. Below is the documentation for the commands related to topics.</p>"},{"location":"admin_API/danube_cli/topics/#commands","title":"Commands","text":""},{"location":"admin_API/danube_cli/topics/#danube-cli-topics-list-namespace","title":"<code>danube-cli topics list NAMESPACE</code>","text":"<p>Get the list of topics in a specified namespace.</p> <p>Usage:</p> <pre><code>danube-cli topics list NAMESPACE\n</code></pre> <p>Description:</p> <p>This command retrieves and displays all topics within a specified namespace. Replace <code>NAMESPACE</code> with the name of the namespace you want to query.</p> <p>Example Output:</p> <pre><code>Topic: topic1\nTopic: topic2\nTopic: topic3\n</code></pre>"},{"location":"admin_API/danube_cli/topics/#danube-cli-topics-create-topic","title":"<code>danube-cli topics create TOPIC</code>","text":"<p>Create a non-partitioned topic.</p> <p>Usage:</p> <pre><code>danube-cli topics create TOPIC\n</code></pre> <p>Description:</p> <p>This command creates a new non-partitioned topic with the specified name. Replace <code>TOPIC</code> with the desired name for the new topic.</p> <p>Example Output:</p> <pre><code>Topic Created: true\n</code></pre>"},{"location":"admin_API/danube_cli/topics/#danube-cli-topics-create-partitioned-topic-topic-partitions","title":"<code>danube-cli topics create-partitioned-topic TOPIC --partitions #</code>","text":"<p>Create a partitioned topic.</p> <p>Usage:</p> <pre><code>danube-cli topics create-partitioned-topic TOPIC --partitions #\n</code></pre> <p>Description:</p> <p>This command creates a new partitioned topic with the specified name and number of partitions. Replace <code>TOPIC</code> with the desired name for the topic and <code>#</code> with the number of partitions.</p> <p>Example Output:</p> <pre><code>Partitioned Topic Created: true\n</code></pre>"},{"location":"admin_API/danube_cli/topics/#danube-cli-topics-delete-topic","title":"<code>danube-cli topics delete TOPIC</code>","text":"<p>Delete a specified topic.</p> <p>Usage:</p> <pre><code>danube-cli topics delete TOPIC\n</code></pre> <p>Description:</p> <p>This command deletes the specified topic. Replace <code>TOPIC</code> with the name of the topic you want to delete.</p> <p>Example Output:</p> <pre><code>Topic Deleted: true\n</code></pre>"},{"location":"admin_API/danube_cli/topics/#danube-cli-topics-unsubscribe-subscription-subscription-topic","title":"<code>danube-cli topics unsubscribe --subscription SUBSCRIPTION TOPIC</code>","text":"<p>Delete a subscription from a topic.</p> <p>Usage:</p> <pre><code>danube-cli topics unsubscribe --subscription SUBSCRIPTION TOPIC\n</code></pre> <p>Description:</p> <p>This command deletes a subscription from a specified topic. Replace <code>SUBSCRIPTION</code> with the name of the subscription and <code>TOPIC</code> with the name of the topic.</p> <p>Example Output:</p> <pre><code>Unsubscribed: true\n</code></pre>"},{"location":"admin_API/danube_cli/topics/#danube-cli-topics-subscriptions-topic","title":"<code>danube-cli topics subscriptions TOPIC</code>","text":"<p>Get the list of subscriptions on a specified topic.</p> <p>Usage:</p> <pre><code>danube-cli topics subscriptions TOPIC\n</code></pre> <p>Description:</p> <p>This command retrieves and displays all subscriptions associated with a specified topic. Replace <code>TOPIC</code> with the name of the topic you want to query.</p> <p>Example Output:</p> <pre><code>Subscriptions: [subscription1, subscription2]\n</code></pre>"},{"location":"admin_API/danube_cli/topics/#danube-cli-topics-create-subscription-subscription-subscription-topic","title":"<code>danube-cli topics create-subscription --subscription SUBSCRIPTION TOPIC</code>","text":"<p>Create a new subscription for a specified topic.</p> <p>Usage:</p> <pre><code>danube-cli topics create-subscription --subscription SUBSCRIPTION TOPIC\n</code></pre> <p>Description:</p> <p>This command creates a new subscription for a specified topic. Replace <code>SUBSCRIPTION</code> with the name of the subscription and <code>TOPIC</code> with the name of the topic.</p> <p>Example Output:</p> <pre><code>Subscription Created: true\n</code></pre>"},{"location":"admin_API/danube_cli/topics/#error-handling","title":"Error Handling","text":"<p>If there is an issue with connecting to the cluster or processing the request, the CLI will output an error message. Ensure your Danube cluster is running and accessible, and check your network connectivity.</p>"},{"location":"admin_API/danube_cli/topics/#examples","title":"Examples","text":"<p>Here are a few example commands for quick reference:</p> <ul> <li>List topics in a namespace:</li> </ul> <pre><code>danube-cli topics list my-namespace\n</code></pre> <ul> <li>Create a non-partitioned topic:</li> </ul> <pre><code>danube-cli topics create my-topic\n</code></pre> <ul> <li>Create a partitioned topic with 5 partitions:</li> </ul> <pre><code>danube-cli topics create-partitioned-topic my-partitioned-topic --partitions 5\n</code></pre> <ul> <li>Delete a topic:</li> </ul> <pre><code>danube-cli topics delete my-topic\n</code></pre> <ul> <li>Unsubscribe from a topic:</li> </ul> <pre><code>danube-cli topics unsubscribe --subscription my-subscription my-topic\n</code></pre> <ul> <li>List subscriptions for a topic:</li> </ul> <pre><code>danube-cli topics subscriptions my-topic\n</code></pre> <ul> <li>Create a new subscription for a topic:</li> </ul> <pre><code>danube-cli topics create-subscription --subscription my-subscription my-topic\n</code></pre> <p>For more detailed information or help with the <code>danube-cli</code>, you can use the <code>--help</code> flag with any command.</p> <p>Example:</p> <pre><code>danube-cli topics --help\n</code></pre>"},{"location":"architecture/PubSub_messaging_vs_Streaming/","title":"Danube Platform","text":"<p>Danube Platform service is designed for high-performance &amp; low-latency messaging. As for now it supports only Pub/Sub Messaging.</p>"},{"location":"architecture/PubSub_messaging_vs_Streaming/#danube-pubsub-messaging","title":"Danube Pub/Sub messaging","text":""},{"location":"architecture/PubSub_messaging_vs_Streaming/#purpose-and-use-cases","title":"Purpose and Use Cases","text":"<ul> <li>Purpose: Designed for decoupling producers and consumers, enabling asynchronous communication between different parts of a system.</li> <li>Use Cases: Event-driven architectures, real-time notifications, decoupled microservices, and distributed systems. Suitable for scenarios where low latency is critical and some message loss is acceptable, such as real-time monitoring, telemetry data, and ephemeral chat messages.</li> </ul>"},{"location":"architecture/PubSub_messaging_vs_Streaming/#architecture-and-design","title":"Architecture and Design","text":"<ul> <li>Components: Consists of Producers, Consumers (subscriptions),  and the message broker.</li> <li>Message Flow: Producers send messages to a broker, which then distributes them to subscribers based on subscription criteria.</li> <li>Scaling: Scales by adding more brokers or distributing load (topics / partitions) across multiple brokers.</li> </ul>"},{"location":"architecture/PubSub_messaging_vs_Streaming/#data-handling-and-processing-models","title":"Data Handling and Processing Models","text":""},{"location":"architecture/PubSub_messaging_vs_Streaming/#pubsub-messaging-producers","title":"Pub/Sub messaging Producers","text":"<ul> <li>Low Latency: Messages sent to topics are not stored on disk, which reduces the latency associated with producing messages.</li> <li>Order Guarantees: Provide ordering within the topic or partition.</li> <li>Message Delivery: There are no guarantees that messages will be delivered. If the broker crashes or if there are network issues, messages might be lost.</li> <li>Transient Acknowledgements: Acknowledgements to the producer are quicker since they are based on in-memory operations rather than disk writes.</li> <li>Publishing Without Consumers: Producers are allowed to publish messages to topics even if there are no active consumers. However, if no consumers are connected, these messages will effectively be dropped because the topics do not store messages.</li> </ul>"},{"location":"architecture/PubSub_messaging_vs_Streaming/#pubsub-messaging-consumers","title":"Pub/Sub messaging Consumers","text":"<ul> <li>Real-time Consumption: Consumers of the topics typically process messages in real-time. If a consumer is not available, the message might be lost.</li> <li>No Replay: Since messages are not stored, consumers cannot replay messages. They must process them as they arrive.</li> <li>Reduced Overhead: The topics can handle higher throughput with lower overhead, suitable for use cases where occasional message loss is acceptable.</li> <li>Message Delivery: Messages are delivered to consumers only if they are currently connected and subscribed to the topic. If there are no consumers, the messages are not retained and are discarded by the broker.</li> </ul>"},{"location":"architecture/PubSub_messaging_vs_Streaming/#order-of-operations-of-pubsub-messaging","title":"Order of Operations of Pub/Sub messaging","text":"<ul> <li>Producer Publishes Message: The producer sends a message to the broker.</li> <li>Broker Receives Message: The broker processes the message.</li> <li>Consumer Availability Check: If consumers are available, the message is delivered to them in real-time.</li> <li>No Consumers: If no consumers are connected, the message is discarded.</li> </ul>"},{"location":"architecture/PubSub_messaging_vs_Streaming/#danube-streaming-design-considerations","title":"Danube Streaming (design considerations)","text":""},{"location":"architecture/PubSub_messaging_vs_Streaming/#purpose-and-use-cases_1","title":"Purpose and Use Cases","text":"<ul> <li>Purpose: Designed for processing and analyzing large volumes of data in real-time as it is generated.</li> <li>Use Cases: Real-time analytics, data pipelines, event sourcing, continuous data processing, and stream processing applications. Ideal for use cases requiring high reliability and message durability, such as financial transactions, order processing, and logging critical events.</li> </ul>"},{"location":"architecture/PubSub_messaging_vs_Streaming/#architecture-and-design_1","title":"Architecture and Design","text":"<ul> <li>Components: Consists of producers, consumers, stream processors, and a distributed log.</li> <li>Data Flow: Producers write data to a distributed log, which consumers and stream processors read from in a continuous fashion.</li> <li>Scaling: Designed to handle high throughput and scale horizontally by partitioning data across multiple nodes in a cluster.</li> </ul>"},{"location":"architecture/PubSub_messaging_vs_Streaming/#data-handling-and-processing-models_1","title":"Data Handling and Processing Models","text":""},{"location":"architecture/PubSub_messaging_vs_Streaming/#streaming-producers","title":"Streaming Producers","text":"<ul> <li>Durability: Messages sent to topics are stored to persistent storage and replicated according to the topic's configuration. This ensures that messages are not lost even if brokers crash. This allows playback of streams for for historical data analysis and reprocessing.</li> <li>Acknowledgements: Producers receive acknowledgments once the message is safely stored and replicated. This adds a small amount of latency compared to pub/sub messaging.</li> <li>Order Guarantees: Provide ordering within the topic or partition.</li> <li>Delivery Guarantees: Producers can rely on stronger delivery guarantees (e.g., at least once or effectively once).</li> <li>Publishing Without Consumers: Producers can publish messages to a topic even if there are no active consumers. These messages will be stored by the broker.</li> <li>Processing: Supports complex processing such as windowed operations, aggregations, joins, and stateful transformations.</li> </ul>"},{"location":"architecture/PubSub_messaging_vs_Streaming/#streaming-consumers","title":"Streaming Consumers","text":"<ul> <li>Message Retention: Consumers can consume messages at any time as long as the retention policy allows. This is useful for replaying messages, ensuring that no messages are missed.</li> <li>Consumption Acknowledgements: Consumers acknowledge each message, allowing the broker to track which messages have been consumed and manage retention accordingly.</li> <li>Fault Tolerance: If a consumer crashes, it can resume consumption from where it left off, as the messages are stored persistently on the broker.</li> <li>Message Retention: Messages are stored according to the configured retention policies. This ensures that even if no consumers are currently connected, the messages will be available for consumption later.</li> </ul>"},{"location":"architecture/PubSub_messaging_vs_Streaming/#order-of-operations-of-streaming","title":"Order of Operations of Streaming","text":"<ul> <li>Producer Publishes Message: The producer sends a message to the broker.</li> <li>Broker Receives and Stores Message: The broker stores the message on the persistent storage and replicates it according to the configuration.</li> <li>Message Acknowledgment: The broker acknowledges the producer that the message is safely stored.</li> <li>Consumer Availability Check: If consumers are available, the message is delivered to them.</li> <li>No Consumers: If no consumers are connected, the message remains stored and is available for future consumption.</li> </ul>"},{"location":"architecture/Queuing_PubSub_messaging/","title":"Pub-Sub messaging","text":"<p>Danube is built on the publish-subscribe pattern. In this pattern, producers publish messages to topics; consumers subscribe to those topics, process incoming messages, and send acknowledgments to the broker when processing is finished.</p>"},{"location":"architecture/Queuing_PubSub_messaging/#messages","title":"Messages","text":"<p>It is the basic unit, they are what producers publish to topics and what consumers then consume from topics.</p> <p>Structure:</p> <ul> <li>Value / data payload - The data carried by the message. The messages should contain raw bytes, and the schema, serialization / deserialization should be managed by producers / consumers.</li> <li>Properties - An optional key/value map of user-defined properties.</li> <li>Producer name - The name of the producer who produces the message.</li> <li>Topic name - The name of the topic that the message is published to.</li> <li>Sequence ID - Each message belongs to an ordered sequence on its topic..</li> <li>Publish time - The timestamp of when the message is published.</li> </ul> <p>The default max size of a message is XX MB, that can be configured.</p>"},{"location":"architecture/Queuing_PubSub_messaging/#acknowledgment","title":"Acknowledgment","text":"<p>A message acknowledgment is sent by a consumer to a broker after the consumer consumes a message successfully. In the Persistent mode the consumed message will be permanently stored and deleted only after all the subscriptions have acknowledged it.</p>"},{"location":"architecture/Queuing_PubSub_messaging/#acknowledgment-timeout","title":"Acknowledgment timeout","text":"<p>In the Persistent mode, the acknowledgment timeout mechanism allows you to set a time range during which the client tracks the unacknowledged messages. After this acknowledgment timeout (ackTimeout) period, the client sends redeliver unacknowledged messages request to the broker, thus the broker resends the unacknowledged messages to the consumer.</p>"},{"location":"architecture/Queuing_PubSub_messaging/#topics","title":"Topics","text":"<p>A topic is a unit of storage that organizes messages into a stream. As in other pub-sub systems, topics are named channels for transmitting messages from producers to consumers. Topic names are URLs that have a well-defined structure:</p>"},{"location":"architecture/Queuing_PubSub_messaging/#namespacetopic_name","title":"/{namespace}/{topic_name}","text":"<p>Example: /default/markets (where default is the namespace and markets the topic)</p>"},{"location":"architecture/Queuing_PubSub_messaging/#subscriptions","title":"Subscriptions","text":""},{"location":"architecture/Queuing_PubSub_messaging/#pub-sub-or-queuing","title":"Pub-Sub or Queuing","text":"<ul> <li>If you want to achieve message queuing among consumers, share the same subscription name among multiple consumers</li> <li>If you want to achieve traditional fan-out pub-sub messaging among consumers, specify a unique subscription name for each consumer with an exclusive subscription type.</li> </ul> <p>A Danube subscription is a named configuration rule that determines how messages are delivered to consumers. It is a lease on a topic established by a group of consumers:</p> <ul> <li>Exclusive (can be used for pub-sub) - The exclusive type is a subscription type that only allows a single consumer to attach to the subscription. If multiple consumers subscribe to a topic using the same subscription, an error occurs.</li> <li>Shared (for queuing) - The shared subscription type Danube allows multiple consumers to attach to the same subscription. Messages are delivered in a round-robin distribution across consumers, and any given message is delivered to only one consumer.</li> </ul>"},{"location":"architecture/Queuing_PubSub_messaging/#multi-topic-subscriptions","title":"Multi-topic subscriptions","text":"<p>Not intended to be supported soon !.. using regex subscription.</p>"},{"location":"architecture/Queuing_PubSub_messaging/#partitioned-topics","title":"Partitioned topics","text":""},{"location":"architecture/Queuing_PubSub_messaging/#not-yet-implemented","title":"Not Yet Implemented","text":"<p>Normal topics are served only by a single broker, which limits the maximum throughput of the topic. Partitioned topic is a special type of topic handled by multiple brokers, thus allowing for higher throughput.</p> <p>A partitioned topic is implemented as N internal topics, where N is the number of partitions. When publishing messages to a partitioned topic, each message is routed to one of several brokers. The distribution of partitions across brokers is handled automatically.</p> <p></p> <p>Messages for the topic are broadcast to two consumers. The routing mode determines each message should be published to which partition, while the subscription type determines which messages go to which consumers.</p>"},{"location":"architecture/Queuing_PubSub_messaging/#routing-modes","title":"Routing modes","text":"<p>When publishing to partitioned topics, you must specify a routing mode. The routing mode determines each message should be published to which partition or which internal topic.</p> <ul> <li>RoundRobinPartition - The producer will publish messages across all partitions in round-robin fashion to achieve maximum throughput. If a key is specified on the message, the partitioned producer will hash the key and assign message to a particular partition.</li> <li>SinglePartition - If no key is provided, the producer will randomly pick one single partition and publish all the messages into that partition. While if a key is specified on the message, the partitioned producer will hash the key and assign message to a particular partition.</li> </ul> <p>=============================</p>"},{"location":"architecture/Queuing_PubSub_messaging/#queuing-vs-pub-sub","title":"Queuing vs Pub-Sub","text":"<p>Below is some general documentation about Queuing and Pub-Sub, not related to Danube implementation, but just to ensure we are aware of the standards.</p> <p>Messaging queuing and pub-sub are both messaging patterns used for asynchronous communication between applications, but they differ in their approach:</p>"},{"location":"architecture/Queuing_PubSub_messaging/#queuing-messaging","title":"Queuing Messaging","text":"<ul> <li>Model: Point-to-Point (One-to-One). A message producer sends a message to a specific queue, and only one consumer can receive and process that message.</li> <li>Order: Messages are typically processed in the order they are received (FIFO - First-In-First-Out). This ensures tasks are completed sequentially.</li> <li>Delivery: Messages are guaranteed to be delivered at least once. This reliability is crucial for critical tasks.</li> </ul> <p>Examples: Use cases include processing orders, sending emails, or handling failed transactions.</p>"},{"location":"architecture/Queuing_PubSub_messaging/#pub-sub-messaging_1","title":"Pub-Sub Messaging","text":"<ul> <li>Model: Publish-Subscribe (One-to-Many). A producer publishes messages to a topic, and any interested subscribers can receive the message. Multiple subscribers can receive the same message.</li> <li>Order: Message order is not guaranteed. Subscribers receive messages as they are published. This is suitable for real-time updates or notifications.</li> <li>Delivery: Delivery is often \"fire-and-forget,\" meaning there's no guarantee a subscriber receives the message. This is acceptable for non-critical data.</li> </ul> <p>Examples: Use cases include broadcasting stock price updates, sending chat messages, or triggering real-time analytics.</p> <p>In Summary:</p> <ul> <li>Messaging queues are for reliable, ordered delivery to a single consumer, ideal for task processing.</li> <li>Pub-sub is for broadcasting messages to many interested parties, good for real-time updates.</li> </ul>"},{"location":"architecture/architecture/","title":"Danube Architecture","text":""},{"location":"architecture/architecture/#danube-pubsub-messaging","title":"Danube Pub/Sub Messaging","text":""},{"location":"architecture/architecture/#only-danube-pubsub-messaging-is-supported-for-the-moment","title":"Only Danube Pub/Sub Messaging is supported for the moment","text":"<p>Designed for decoupling producers and consumers, enabling asynchronous communication between different parts of a system. Suitable for scenarios where low latency is critical and some message loss is acceptable, such as real-time monitoring / notifications, telemetry data, event-driven architectures.</p> <p>The messages reside only in memory, providing low latency but not guaranteed to survive broker crashes or consumer disconnections.  The producers are allowed to send messages to Topics even if there are no active consumers. If no consumers are found the messages are droped.</p> <p>Read Here for more detailed design considerations.</p> <p></p>"},{"location":"architecture/architecture/#brokers","title":"Brokers","text":"<p>A cluster consist of one or more Danube Brokers.</p> <p>The producers connect to the brokers to publish messages and the consumers connect to the brokers to consume the messages.</p> <p>Messages are dispatched immediatelly to available consumers, for increased performance.</p>"},{"location":"architecture/architecture/#metadatastore","title":"MetadataStore","text":"<p>Used for cluster-level metadata storage, configuration and coordination. Maintain the metadata store of the Danube Cluster, such as namespace / topic metadata, broker load data and others.</p>"},{"location":"architecture/architecture/#danube-stream-not-yet-supported","title":"Danube Stream (not yet supported)","text":"<p>Designed for processing and analyzing large volumes of data in real-time as it is generated.</p> <p>In Danube Stream the messages are stored durably on disk (across multiple disks for reliability). This ensures message survival even during broker restarts or consumer failures. Ideal for use cases requiring high reliability and message durability, such as financial transactions, order processing, logging critical events, etc.</p> <p></p>"},{"location":"architecture/architecture/#_brokers","title":"_Brokers","text":"<p>A cluster consist of one or more Danube Brokers.</p> <p>The producers connect to the brokers to publish messages and the consumers connect to the brokers to consume the messages.</p> <p>Messages are typically dispatched out of a in memory cache for the sake of performance. If the backlog grows too large for the cache, the broker will start reading entries from the distributed storage.</p>"},{"location":"architecture/architecture/#_metadatastore","title":"_MetadataStore","text":"<p>Used for cluster-level metadata storage, configuration and coordination. Maintain the metadata store of the Danube Cluster, such as namespace / topic metadata, broker load data and others.</p>"},{"location":"architecture/architecture/#message-store","title":"Message Store","text":"<p>It provides message delivery guarantee for applications. If a message successfully reaches the Danube broker, it will be delivered to its intended target.</p> <p>This guarantee requires that non-acknowledged messages are stored durably until they can be delivered to and acknowledged by consumers. This mode of messaging is commonly called persistent messaging.</p>"},{"location":"architecture/internal_danube_services/","title":"Danube Cluster Services Role","text":"<p>This document enumerates the principal components of the Danube Cluster and their responsibilities.</p>"},{"location":"architecture/internal_danube_services/#danube-service-components","title":"Danube Service Components","text":""},{"location":"architecture/internal_danube_services/#leader-election-service","title":"Leader Election Service","text":"<p>The Leader Election Service selects one broker from the cluster to act as the Leader. The Broker Leader is responsible for making decisions. This service is used by the Load Manager, ensuring only one broker in the cluster posts the cluster aggregated Load Report.</p> <p>Leader Election Flow:</p> <ul> <li>The first broker registered in the cluster becomes the Leader by registering itself on \"/cluster/leader\".</li> <li>The field is registered with a lease, so the leader broker must periodically renew its lease to maintain leadership.</li> <li>Subsequent brokers attempt to become leaders but become Followers if the path is already in use.</li> <li>All brokers periodically check the leader path. If there is no change, the state is maintained; otherwise, brokers attempt to become the leader.</li> </ul>"},{"location":"architecture/internal_danube_services/#load-manager-service","title":"Load Manager Service","text":"<p>The Load Manager monitors and distributes load across brokers by managing topic and partition assignments. It implements rebalancing logic to redistribute topics/partitions when brokers join or leave the cluster and is responsible for failover mechanisms to handle broker failures.</p> <p>Load Manager Flow:</p> <ul> <li>All brokers periodically post their Load Reports on the path \"/cluster/brokers/load/{broker-id}\".</li> <li>The leader broker watches for load reports from all brokers in the cluster.</li> <li>It calculates rankings using the selected Load Balance algorithm.</li> <li>It posts its calculations for the cluster on the \"/cluster/load_balance\" path.</li> </ul> <p>Creation of a New Topic:</p> <ul> <li>A broker registers the Topic on the \"/cluster/unassigned\" path.</li> <li>The Load Manager of the leader Broker watches this path and assigns the broker with the least load to host the new topic by posting the topic to the \"/cluster/brokers/{broker-id}/{topic_name}\" path.</li> <li>Each broker watches its own path: \"/cluster/brokers/{broker-id}\". For any event on that path, such as the addition or deletion of topics, it acts accordingly by creating a new topic locally or deleting the topic it owned and all related resources.</li> <li>On topic creation, the broker checks if the topic already exists locally. If not, it retrieves all data about the topic, including subscriptions and producers, from the Local Metadata Cache.</li> <li>On topic removal, the broker handles the disconnections of producers and consumers and removes the locally allocated resources.</li> </ul> <p>For further consideration: We may want the broker to ask the Load Manager to get the next broker and initiate topic creation. Either it just posts the topic on the \"/cluster/unassigned\" path, or if it is the selected broker, it also creates the topic locally.</p>"},{"location":"architecture/internal_danube_services/#local-metadata-cache","title":"Local Metadata Cache","text":"<p>This cache stores various types of metadata required by Danube brokers, such as topic and namespace data, which are frequently accessed during message production and consumption. This reduces the need for frequent queries to the central metadata store, ETCD.</p> <p>The docs/internal_resources.md document describes how the resources are organized in the Metadata Store.</p> <p>Updates/events are received via ETCD Watch events and/or the metadata event synchronizer.</p>"},{"location":"architecture/internal_danube_services/#syncronizer","title":"Syncronizer","text":"<p>The synchronizer ensures that metadata and configuration settings across different brokers remain consistent. It propagates changes to metadata and configuration settings using client Producers and Consumers.</p> <p>This is in addition to Metadata Storage watch events, allowing brokers to process metadata updates even if there was a communication glitch or the broker was unavailable for a short period, potentially missing the Store Watch events. The synchronizer allows for dynamic updates to configuration settings without requiring a broker service restart.</p>"},{"location":"architecture/internal_danube_services/#danube-broker","title":"Danube Broker","text":"<p>The Broker owns the topics and manages their lifecycle. It also facilitates the creation of producers, subscriptions, and consumers, ensuring that producers can publish messages to topics and consumers can consume messages from topics.</p>"},{"location":"architecture/internal_danube_services/#external-metadata-storage-etcd","title":"External Metadata Storage (ETCD)","text":"<p>This is the Metadata Storage responsible for the persistent storage of metadata and cluster synchronization.</p>"},{"location":"client_libraries/Go_client/setup/","title":"Danube Pub/Sub Go client","text":"<p>The client is still in development</p>"},{"location":"client_libraries/Rust_client/setup/","title":"Danube Pub/Sub Rust client","text":"<p>ToDo</p>"},{"location":"client_libraries/Rust_client/usage/","title":"Use Danube Rust cient","text":"<p>ToDo</p>"},{"location":"development/dev_environment/","title":"Development Environment Setup for Danube Broker","text":"<p>This document guides you through setting up the development environment, running danube broker instances, and be able to effectively contribute to the code.</p>"},{"location":"development/dev_environment/#prerequisites","title":"Prerequisites","text":"<p>Before you get started, make sure you have the following installed:</p> <ul> <li> <p>Rust: Ensure you have Rust installed. You can download and install it from the Rust website.</p> </li> <li> <p>Docker: Install Docker if you haven\u2019t already. Follow the installation instructions on the Docker website.</p> </li> </ul>"},{"location":"development/dev_environment/#contributing-to-the-repository","title":"Contributing to the Repository","text":"<ol> <li> <p>Fork the Repository:</p> </li> <li> <p>Go to the Danube Broker GitHub repository.</p> </li> <li> <p>Click the \"Fork\" button on the top right corner of the page to create your own copy of the repository.</p> </li> <li> <p>Clone Your Fork:</p> </li> </ol> <p>Once you have forked the repository, clone your forked repository:</p> <pre><code>git clone https://github.com/&lt;your-username&gt;/danube.git\ncd danube\n</code></pre> <ol> <li>Add the Original Repository as a Remote (optional but recommended for keeping up-to-date):</li> </ol> <pre><code>git remote add upstream https://github.com/danrusei/danube.git\n</code></pre>"},{"location":"development/dev_environment/#building-the-project","title":"Building the Project","text":"<ol> <li>Build the Project:</li> </ol> <p>To build the Danube Broker:</p> <pre><code>cargo build \nor  \ncargo build --release\n</code></pre>"},{"location":"development/dev_environment/#running-etcd","title":"Running ETCD","text":"<ol> <li>Start ETCD:</li> </ol> <p>Use the Makefile to start an ETCD instance. This will run ETCD in a Docker container.</p> <pre><code>make etcd\n</code></pre> <ol> <li>Clean Up ETCD:</li> </ol> <p>To stop and remove the ETCD instance and its data:</p> <pre><code>make etcd-clean\n</code></pre>"},{"location":"development/dev_environment/#running-a-single-broker-instance","title":"Running a Single Broker Instance","text":"<ol> <li>Start ETCD:</li> </ol> <p>Ensure ETCD is running. If not, use the <code>make etcd</code> command to start it.</p> <ol> <li>Run the Broker:</li> </ol> <p>Use the following command to start a single broker instance:</p> <pre><code>RUST_LOG=danube_broker=trace target/debug/danube-broker --cluster-name MY_cluster --meta-store-addr 127.0.0.1:2379\n</code></pre>"},{"location":"development/dev_environment/#running-multiple-broker-instances","title":"Running Multiple Broker Instances","text":"<ol> <li>Start ETCD:</li> </ol> <p>Ensure ETCD is running. Use:</p> <pre><code>make etcd\n</code></pre> <ol> <li>Run Multiple Brokers:</li> </ol> <p>Use the following Makefile command to start multiple broker instances:</p> <pre><code>make brokers\n</code></pre> <p>This will start brokers on ports 6650, 6651, and 6652. Logs for each broker will be saved in <code>temp/</code> directory.</p> <ol> <li>Clean Up Broker Instances:</li> </ol> <p>To stop all running broker instances:</p> <pre><code>make brokers-clean\n</code></pre>"},{"location":"development/dev_environment/#reading-logs","title":"Reading Logs","text":"<p>Logs for each broker instance are stored in the <code>temp/</code> directory. You can view them using:</p> <pre><code>cat temp/broker_&lt;port&gt;.log\n</code></pre> <p>Replace <code>&lt;port&gt;</code> with the actual port number (6650, 6651, or 6652).</p>"},{"location":"development/dev_environment/#inspecting-etcd-metadata","title":"Inspecting ETCD Metadata","text":"<ol> <li>Set Up <code>etcdctl</code>:</li> </ol> <p>Export the following environment variables:</p> <pre><code>export ETCDCTL_API=3\nexport ETCDCTL_ENDPOINTS=http://localhost:2379\n</code></pre> <ol> <li>Inspect Metadata:</li> </ol> <p>Use <code>etcdctl</code> commands to inspect metadata. For example, to list all keys:</p> <pre><code>etcdctl get \"\" --prefix\n</code></pre> <p>To get a specific key:</p> <pre><code>etcdctl get &lt;key&gt;\n</code></pre>"},{"location":"development/dev_environment/#makefile-targets-summary","title":"Makefile Targets Summary","text":"<ul> <li><code>make etcd</code>: Starts an ETCD instance in Docker.</li> <li><code>make etcd-clean</code>: Stops and removes the ETCD instance and its data.</li> <li><code>make brokers</code>: Builds and starts broker instances on predefined ports.</li> <li><code>make brokers-clean</code>: Stops and removes all running broker instances.</li> </ul>"},{"location":"development/dev_environment/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>ETCD Not Starting: Check Docker logs and ensure no other service is using port 2379.</li> <li>Broker Not Starting: Ensure ETCD is running and accessible at the specified address and port.</li> </ul>"},{"location":"development/internal_resources/","title":"Resources mapping","text":"<p>This document describes how the resources are organized in the Metadata store</p>"},{"location":"development/internal_resources/#metadatastore-and-localcache","title":"MetadataStore and LocalCache","text":"<p>Basically the entire configuration and the metadata for all the cluster's objects (topics, namespaces, etc) are stored in MetadataStorage (ETCD) and in the LocalCache to ensure fast retrieval for the local broker and to reduce the number of request to the metadata database.</p> <p>The pattern:</p> <ul> <li>Put / Delete requests should use MetadataStore (ETCD), to ensure consistency across cluster</li> <li>Get requests should be served from the Local Cache</li> </ul> <p>The LocalCache continuously update from 2 sources for increase consistency:</p> <ul> <li>the Watch operation on ETCD</li> <li>the Syncronizer topic, where all Put / Delete requests are published and read by the brokers.</li> </ul>"},{"location":"development/internal_resources/#resources-types","title":"Resources Types","text":""},{"location":"development/internal_resources/#cluster-resources","title":"Cluster Resources","text":"<p>Holds information about the cluster and the cluster's brokers. Mainly read and write by Danube Service.</p> <ul> <li>/cluster/cluster-name</li> <li>holds a String with the name of the cluster</li> <li>/cluster/register/{broker-id}</li> <li>the broker register once it join the cluster, contain the broker metadata (broker id &amp; socket addr)  </li> <li>/cluster/brokers/{broker-id}/{namespace}/{topic}</li> <li>topics served by the broker, with value ()</li> <li>Load Manager updates the path, with topic assignments to brokers</li> <li>Brokers should watch it's own path like (/cluster/brokers/1122334455) - and perform the neccesary actions on adding or removing a topic</li> <li>/cluster/unassigned/{namespace}/{topic}</li> <li>New unassigned topics created by Broker</li> <li>Load Manager should watch this path, add assign the topic to a broker</li> <li>/cluster/load/{broker-id}</li> <li>broker periodically reports its load metrics on this path</li> <li>Load Manager watch this path to calculate broker load rankings for the cluster</li> <li>/cluster/load_balance</li> <li>the load_balance updated decision, posted by the Load Manager, contain a HashMap with keys the broker_id and value the list of topic_name</li> <li>/cluster/leader</li> <li>the value posted by Leader Election service, it holds broker_id of the current Leader of the CLuster</li> </ul> <p>Example:</p> <ul> <li>/cluster/brokers/1122334455/markets/trade-events - value is ()</li> <li>/cluster/brokers/1122334455/markets/trade-events - value is ()</li> </ul>"},{"location":"development/internal_resources/#namespace-resources","title":"Namespace Resources","text":"<p>Holds information about the namespace policy and the namespace's topics</p> <ul> <li>/namespaces/{namespace}/policy</li> <li>/namespaces/{namespace}/topics/{namespace}/{topic}</li> </ul> <p>Example:</p> <ul> <li>/namespaces/markets/policy - the value stores a Json like { \"retentionTimeInMinutes\": 1440 }</li> <li>/namespaces/markets/topics/markets/trade-events - topics part of the namespace, value is ()</li> </ul>"},{"location":"development/internal_resources/#topic-resources","title":"Topic Resources","text":"<p>Holds information about the topic policy and the associated producers / subscriptions, including partitioned topic.</p> <ul> <li>/topics/{namespace}/{topic}/policy</li> <li>holds the topic policy, the value stores a Json</li> <li>/topics/{namespace}/{topic}/schema</li> <li>holds the topic schema, the value stores the schema</li> <li>/topics/{namespace}/{topic}/producers/{producer_id}</li> <li>holds the producer config</li> <li>/topics/{namespace}/{topic}/subscriptions/{subscription_name}</li> <li>holds the subscription config</li> </ul> <p>Example:</p> <ul> <li>/topics/markets/trade-events/producers/1122334455 - with value Producer Metadata</li> <li>/topics/markets/trade-events/subscriptions/my_subscription - with value Subscription Metadata</li> <li>/topics/markets/trade-events-part-1/policy - where /markets/trade-events-part-1 is the partitioned topic that stores partition policy</li> </ul>"},{"location":"development/internal_resources/#subscriptions-resources","title":"Subscriptions Resources","text":"<p>Holds information about the topic subscriptions, including associated consumers</p> <ul> <li>/subscriptions/{subscription_name}/{consumer_id}</li> <li>holds the consumer metadata</li> </ul> <p>Example:</p> <ul> <li>/subscriptions/my_subscription/23232323</li> </ul>"},{"location":"getting_started/producers_consumers/","title":"Danube Clients","text":""},{"location":"getting_started/producers_consumers/#client-setup","title":"Client Setup","text":"<p>Before an application creates a producer/consumer, the  client library needs to initiate a setup phase including two steps:</p> <ul> <li>The client attempts to determine the owner of the topic by sending a Lookup request to Broker.  </li> <li>Once the client library has the broker address, it creates a RPC connection (or reuses an existing connection from the pool) and (in later stage authenticates it ).</li> <li>Within this connection, the clients (producer, consumer) and brokers exchange RPC commands. At this point, the client sends a command to create producer/consumer to the broker, which will comply after doing some validation checks.</li> </ul> <p>TODO: Reconnection with exponential backoff .Whether the TCP connection breaks, the client immediately re-initiates this setup phase and keeps trying to re-establish the producer or consumer until the operation succeeds.</p> <p></p>"},{"location":"getting_started/producers_consumers/#producer","title":"Producer","text":"<p>A producer is a process that attaches to a topic and publishes messages to a Danube broker. The Danube broker processes the messages.</p> <p>Access Mode is a mechanism to determin the permissions of producers on topics.</p> <ul> <li>Shared - Multiple producers can publish on a topic.</li> <li>Exclusive - If there is already a producer connected, other producers trying to publish on this topic get errors immediately.</li> </ul>"},{"location":"getting_started/producers_consumers/#example","title":"Example","text":"<pre><code>let client = DanubeClient::builder().service_url(\"http://[::1]:6650\").build().unwrap();\nlet topic = \"/markets/trade-events\".to_string(); // Topics are defined as /{namespace}/{tiopic_name}\n\nlet json_schema = r#\"{\"type\": \"object\", \"properties\": {\"field1\": {\"type\": \"string\"}, \"field2\": {\"type\": \"integer\"}}}\"#.to_string();\n\nlet mut producer = client.new_producer().with_topic(topic).with_name(\"test_producer1\")\n        .with_schema(\"my_app\".into(), SchemaType::Json(json_schema)).build();\nproducer.create().await?;\n\nlet data = json!({\n            \"field1\": format!{\"value{}\", i},\n            \"field2\": 2020+i,\n        });\n\n// Convert to string and encode to bytes\nlet json_string = serde_json::to_string(&amp;data).unwrap();\nlet encoded_data = json_string.as_bytes().to_vec();\n producer.send(encoded_data).await?;\n</code></pre>"},{"location":"getting_started/producers_consumers/#consumer","title":"Consumer","text":"<p>A consumer is a process that attaches to a topic via a subscription and then receives messages.</p> <p>Subscription Types - describe the way the consumers receive the messages from topics</p> <ul> <li>Exclusive -  Only one consumer can subscribe, guaranteeing message order.</li> <li>Shared -  Multiple consumers can subscribe, messages are delivered round-robin, offering good scalability but no order guarantee.</li> <li>Failover - Similar to shared subscriptions, but multiple consumers can subscribe, and one actively receives messages.</li> </ul>"},{"location":"getting_started/producers_consumers/#example_1","title":"Example","text":"<pre><code>let client = DanubeClient::builder().service_url(\"http://[::1]:6650\").build().unwrap();\nlet topic = \"/markets/trade-events\".to_string(); // Topics are defined as /{namespace}/{tiopic_name}\n\nlet mut consumer = client.new_consumer().with_topic(topic).with_consumer_name(\"test_consumer\")\n        .with_subscription(\"test_subscription\").with_subscription_type(SubType::Exclusive).build();\n\n// Subscribe to the topic\nconsumer.subscribe().await?;\n\n// Start receiving messages\nlet mut message_stream = consumer.receive().await?;\nwhile let Some(message) = message_stream.next().await {\n    //process the message and ack for receive\n}\n</code></pre>"}]}